As a normal process, we're going to want to be able to use live site snapshots for testing & validation of upgrades, as a playground to experiment with new features etc.

This will generally involve a little bit of fiddling in order to make a live site instance, but the basic order of operations is:

1. backup the live site volumes
2. restore the live site volumes
3. modify restored-volume ownerships / permissions (as required)
4. spin up the the new volumes in your test bed
5. retarget the localhost test site
6. spin up your retargeted site (right round baby right round)

We'll go through each of these steps in depth below
# Step 1 - Backup Live Site Volumes

Frappe uses volumes to store data persistently between spin-ups / contained instances. As such, the core of our ability to duplicate sites (including their database schema & information) comes from our ability to duplicate volumes.
## Breakdown of Volumes used in Frappe

A Frappe site has the following volumes
1. **Site -** `{project-name}_sites` - contains site config data along with restic backups, public/private files, and more. This is a volume of critical importance because it contains our db credentials without which accessing our data is impossible.
2. **Database -** `{project-name}_db-data` - contains our actual mysql database tables, unlike all other volumes, this gets mounted separately to a container running a mariadb image. Frappe then communicates with this container to do actual DB access tasks.
3. **Logs -** `{project-name}_logs` - contains system logs (generally not viewable at the desk level) generated by the system (or our custom code). 
4. **Redis Cache -** `{project-name}_redis-cache-data` - Frappe uses redis as a caching service to speed up db access tasks & queue things up.
5. **Redis Queue -** `{project-name}_redis-queue-data` - See the above
6. **Anonymous Volumes** - `{incomprehensible_string_of_digits}` - for reasons that still aren't perfectly clear to me, `sites/assets` is mounted as a separate volume. This is a symlink to each of the installed apps on the site in order to access their custom assets. These volumes are non-persistent, and aren't automagically destroyed. As such, when you `compose down`, it's recommended that you run `docker volume prune` to delete these anonymous volumes (since they can't be used again anyways)

Of these volumes, we only care about (and will only use) the site & database volumes. The reasons for this are
- **logs** - aren't useable due to weird ownership issues, and we don't necessarily want live site logs in our test bed because we want to only have the logs we generate in the test bed when we're reviewing things in the test bed
- **redis -** nothing critical exists in the cache / queue volumes and rebuilding isn't an issue
- **anonymous volumes -** as mentioned above, can't be reused.
## Backup Procedure

So to back up a site volume, go to the location (directory) you'd like to back up your volumes to. Then you'll use the `volume-archive.sh` script located in `{de-erpnext repo location}/tools/volume-restore/`. Full usage & explanation of that script is available in `volume_cloning.md` (located in the same "documentation" directory as this). 

Suffice it to say, you need to be backing up both the side & database volumes. 

> [!WARNING]
> If possible, it's preferable to spin down the site before you backup your volumes. This is because there is a possibility of saving the database in the midst of a db transaction, possibly leading to corrupted data. 
> 
> That said, in my personal experience the system is fairly fault tolerant, so so long as you're not actively in the midst of an operation during the backup process you might get a warning the first time you spin up the restored volume, it'll resolve itself & you should be fine

# Step 2 - Restore Live Site Volumes

The docker compose process will "fill in the gaps" on any missing volumes we don't have, as such we're only interested in restoring our 2 primary volumes (sites & database). 
## Restoration Procedure
As above, more thorough documentation about this process is available in `volume_cloning.md`,  but in brief we'll be using the `volume-restorer.sh` script to create our volumes for use. That said, this process is pretty straight-forward


> [!TIP] 
> For good labelling and convenience it's a REALLY good idea to use the optional second argument in the volume restoration utility to set a custom project name for the restored volumes. You'll then use this custom project name during the steps 4 & 6 spin ups

# Step 3 - Modify Live Site Volume Permissions

There is a bug / behavior that I've been unable to source (though I believe it's a function of the tar unpacking process of step 2): the ownership of the root directory of the volume can get reset to `root`. 

The symptoms of this bug will be that when you try to spin up the site through the compose process, you'll get your frontend / backend containers timing out & exiting with code 1 along with access denied errors (for site volume permission errors), or db access errors (for bad ownership of the db root directory).

The ownership of the top level directory of the site & database volumes are as follows
- sites - `_data` owner should be `1000:1000`
- database - `_data` owner should be `999:docker`


> [!IMPORTANT]
> Docker volumes by default are stored in `/var/lib/docker/volumes`, by default you'll be denied access to those folders, so you can either `sudo su` your way in, or you can just specify the long path.
> 
> Either way, you'll be using the `chown` command to correct the bad ownership. So for a bad site ownership that would look something like:
> ```bash
> sudo chown 1000:1000 /var/lib/docker/volumes/{project-name}_sites
> ```

Once you've verified that these ownerships are correct, you can move on to step 4.
# Step 4 - Spin-Up Volumes 

The goal of this step is to generate a localhost directed site that we'll then reconfigure to point to our live-site database.

This requires one TINY setup step first. You need to enter your `.env` file & update the `FRAPPE_SITE_NAME_HEADER` and `SITE` fields to be a localhost site (so something like `testserve.localhost`). This will be the site name you'll be using for your testing / exploration, so be sure to set it to something descriptive & useful.

From there you'll pop over to your `de-erpnext` local repo clone & run your first spin-up, using the command
```bash
docker compose -p {project-name} -f compose-main.yaml up
```

some things to note here:
1. `-p {project-name}` - needs to match the project-name used by your restored site volumes. If not, your  volumes aren't being targeted and this was all for naught.
2. `-f compose-main.yaml` - we're assuming that you've already created / imported your image and have configured your `compose-main.yaml` (and `.env`) file appropriately. The configuration of those two items is covered separately.
3. not using the `-d` flag - when we're setting up our site to be running for a long period of time, we'll commonly append the `-d` flag to run our compose in detached mode in order to have the containers run independently of any terminal instance. In our case, we want to be able to view output (and will only be running for a few minutes at most) so we'll purposely omit this flag.

When you run the spin-up you should see the console start spitting out container logs as the containers start up (and in the case of `create_site` and `configurator` shut back down). You may get some error messages, but by and large, the process should simply proceed. 

> [!CAUTION]
> If you get any containers (especially frontend or backend) exiting with status code 1, that means something has gone wrong in the process. Most likely culprit is bad permissions, purge the volumes and restart, either from step 3 or from step 2.

You'll know you can move forward once the `create_site` volume exits after noting it has made the `{site-name}.localhost` site you designated in your `.env` file successfully. Once you've received this beautiful message, spin down with the same command as spin up, save that you're spinning down

# Step 5 - Retarget DB config info

So here comes the tricky part where we do something a bit nefarious. Mount up your site volume (anything will do), I commonly use alpine because it's lightweight and easy. Navigate to your `sites` directory for the given sites volume.

You should see a common config and a few directories corresponding to your site names.

First things first, go into your live-site directory & copy down the info located in your `site_config.json` for that site. The fields of particular interest are
- `db_name`
- `db_password`
- `db_type` (should always be mariadb, but just in case)
- `encryption_key`

What you're then going to do it copy these values into the `site_config.json` for your localhost site. Just override the values that exist (and you'll likely have to add the `encrpytion_key` member). The next time you spin up the site, the localhost site will access the live-site instanced database. 

You can now safely get rid of whatever temporary container you just built to do this volume editing.
# Step 6 - Go forth & conquer

Now all that's left to do is spin up your `compose-main.yaml` again (probably still omit the `-d` flag for the first go round since we want to see if mariadb throws any access errors in the docker logs). This time your localhost site will be targeting your copied live site database information. You can log in exactly as you would to the live-site (including the same credentials) and experiment to your heart's content.